---
title: "Intro to clustering and dimensionality reduction"
author: "Fergal Duffy"
date: "4/26/2022"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
```

# Introduction

This tutorial will give a gentle introduction to some dimensionality reduction and clustering approaches in R

- Dimensionality reduction
  - Principle component analysis (PCA)
  - UMAP
- Clustering
  - k-means clustering
  - hierarchical clustering
  
All code is available on the i4TB github [https://github.com/fergaljd/i4TB](https://github.com/fergaljd/i4TB)


### Sample Dataset

We will get our data from NCBI GEO: a microarray dataset of whole blood transcriptome profiles from
active TB patients and healthy controls from the UK originally published in the paper:

> Berry MP, Graham CM, McNab FW, Xu Z et al. An interferon-inducible neutrophil-driven blood transcriptional signature in human tuberculosis. Nature 2010 Aug 19;466(7309):973-7. PMID: 20725040

The R GEOquery package lets us automate this. We download the dataset from GEO and save it to a folder.This consists of normalized
microarray gene intensity data. Here, we set negative expression values to 1, log transform. To speed the analysis here, we are limiting our analysis to only the 100 most variable genes, using coefficient of variation (sd(expression)/mean(expression)).


```{r}
library("ggplot2")
library("tidyverse")
library("ggrepel")
library("umap")
library("pheatmap")
library("GEOquery")
```

```{r cache=T}

#TB vs healthy control microarray data

dir.create("./i4tb-clustering-data/")
gse19435 <- getGEO("GSE19435", destdir = "./i4tb-clustering-data/")

#Lets create an annotated expression matrix, with rows in samples and variables in columns
#To speed the analysis, we will limit ourselves to the top 100 most variable genes
gse19435exprMat <- exprs(gse19435[[1]]) + abs(min(exprs(gse19435[[1]]))) + 1


nGenesToPick <- 100
mostVariableGenes <- names(sort(apply(gse19435exprMat, 1, function(x)sd(x)/mean(x)), decreasing =TRUE)[1:nGenesToPick])
```

Now, we create a new data frame with 100 columns of gene expression, a sample ID column, and
a Status column, indicating TB or Control. This also involves converting the microarray probe IDs
to more interpretable gene names.

The `expr` function pulls out gene expression,
the `pData` function pulls out sample metadata (phenotype data) and the `fData` function maps 
illumina microarray probe IDs to gene names and descriptiptions (feature data).

We are also making use of the `dplyr` pipe function `%>%` which passes the result of the previous
function into the first argument of the next function. This allows us to chain together multiple functions
in a readable way, we don't need multiple levels of nested parentheses.



```{r}
tbVsControlExprMat <- gse19435exprMat[mostVariableGenes,] %>% 
  data.frame() %>% 
  rownames_to_column("ILMN_ID") %>%
  left_join(select(fData(gse19435[[1]]), ILMN_ID=ID, Gene=ILMN_Gene)) %>%
  mutate(Gene=make.unique(Gene)) %>%
  select(-ILMN_ID) %>%
  pivot_longer(-Gene, names_to="SampleID", values_to="expr") %>%
  mutate(expr=ifelse(expr<=1, 0, log2(expr))) %>%
  pivot_wider(names_from=Gene, values_from=expr) %>%
  left_join(select(pData(gse19435[[1]]), SampleID="geo_accession", Status="illness:ch1"))

```


# Dimensionality reduction

Large omics datasets are often high-dimensional, measuring tens of thousands of genes/proteins/cells. It's not practical to
visualize all of these measurements simultaneously. However, biology makes use of coherent response pathways. For example, many genes are expressed in a correlated fashion in response to a stimulus.

Dimensionality reduction is a set of techniques that aim to collapse down high-dimensional measurements into a lower dimensional representation that captures the important biological variability of interest. 

For a concrete example, let's take a look at principle component analysis (**PCA**)

## PCA
PCA performs a linear transformation on a set of possibly correlated variables to produce a new set of uncorrelated variables. PCA will always create the first principle component to account for as much variability as possible, the second 
principle component to account for as much of the remaining variability as possible, and so on until all variability has been captured.

Let's explore PCA with a simple example of 2 genes from the TB microarray dataset. **GBP5** and **OAS3** are both interferon-induced genes that are increased in active TB. Plotting their expression reveals that their expression is tightly correlated in this dataset.


```{r}

twoGeneCor <- cor.test(~GBP5+OAS3, data=tbVsControlExprMat)

ggplot(tbVsControlExprMat, aes(x=GBP5, y=OAS3)) +
    stat_smooth(method="lm", se=F, colour="black") +
  geom_point(aes(colour=Status), size=3) +
  theme_bw() +
  annotate("text", x=11.5, y=8, size=8,
           label=sprintf('r==%.2f', twoGeneCor$estimate), parse=T) 

```

PCA calculates a linear transformation of the data. This creates new linear combinations of the original GBP5 and OAS3 expression,
that are uncorrelated. 

The `prcomp` function calculates the PCA transform, and the returned PCA object keeps the transformed data in the `x` slot.

The rotation matrix is stored in the `rotation` slot.

We can also calculate the variance explained by each PC, as a fraction of the total variance. 
Remember, variance = stdev^2


```{r}
twoGenePCA <- prcomp(tbVsControlExprMat[c("GBP5", "OAS3")])

twoGenePCAdf <- twoGenePCA$x %>%
  data.frame(Status=tbVsControlExprMat$Status) %>%
  mutate(Status=tbVsControlExprMat$Status)

twoGenePCArotationDF <- twoGenePCA$rotation %>%
  data.frame() %>%
  rownames_to_column("gene")

twoGeneVarianceExplained <- ((twoGenePCA$sdev^2) / sum(twoGenePCA$sdev^2)) %>%
  set_names(sprintf("PC%d", 1:length(.)))

```
Here we see that PC1 explains about `r signif(twoGeneVarianceExplained["PC1"]*100, 2)`% of the variance in our two genes,
and we can visualize where the genes fit into PCA space

```{r}

ggplot(twoGenePCAdf, aes(x=PC1, y=PC2)) +
  geom_point(aes(colour=Status), size=3) +
  theme_bw() +
  geom_segment(data=twoGenePCArotationDF, aes(x=0, y=0, xend=PC1, yend=PC2), 
               arrow=arrow(type="closed", length = unit(0.1, "inches"))) +
  geom_text(data=twoGenePCArotationDF, aes(x=PC1*1.1, y=PC2*1.1, label=gene)) +
  labs(title="PCA transformed GBP5/OAS3",
       x=sprintf("PC1 (%.2f%%)", twoGeneVarianceExplained["PC1"]*100),
       y=sprintf("PC2 (%.2f%%)", twoGeneVarianceExplained["PC2"]*100))
```

Now, what happens if we PCA transform all the genes?


```{r}
tbVsControlPCA <- prcomp(select(tbVsControlExprMat, -Status, -SampleID))

tbVsControlPCAdf <- tbVsControlPCA$x %>%
  data.frame() %>%
  mutate(SampleID=tbVsControlExprMat$SampleID,
         Status=tbVsControlExprMat$Status)

tbVsControlVarianceExplained <- ((tbVsControlPCA$sdev^2) / sum(tbVsControlPCA$sdev^2)) %>%
  set_names(sprintf("PC%d", 1:length(.)))
```

When we plot the first two PCs, we can see that we have summarized `r signif(tbVsControlVarianceExplained["PC1"]*100, 2)`% + `r signif(tbVsControlVarianceExplained["PC2"]*100, 2)`% = `r signif(sum(tbVsControlVarianceExplained[1:2])*100, 2)`%  of the variance in our 100
gene dataset in just two PCs

We also see good separation between control and PTB samples, which implies that a large proportion of the variation in this dataset is driven by TB infection

```{r}
ggplot(tbVsControlPCAdf, aes(x=PC1, y=PC2, colour=Status)) +
  geom_point(size=3) +
  theme_bw() +
  labs(title="PCA transformed 100 most variable genes",
       x=sprintf("PC1 (%.2f%%)", tbVsControlVarianceExplained["PC1"]*100),
       y=sprintf("PC2 (%.2f%%)", tbVsControlVarianceExplained["PC2"]*100))


```

## UMAP

PCA linearly transforms your data to a set of independent 'principle components'

Another approach to visualizing your data is approaches such as UMAP and t-SNE. UMAP and t-SNE
flatten out variation in your data to 2 (or more) dimensions, putting points 'close' together in high-dimensional
space close together in 2D space. However, in order to reduce a high dimensional representation into a low dimensional one,
UMAP 'warps' the data. UMAP is also stochastic, meaning you get slightly different results each time.

How UMAP calculates this transform is complex, but for an intuitive explanation take a look at
[https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/)

Lets apply UMAP to our TB dataset, using the `umap` function from the `umap` package

```{r}

tbVsControlUmap <- umap(select(tbVsControlExprMat, -Status, -SampleID))

tbVsControlUmapDF <- data.frame(tbVsControlUmap$layout) %>%
  mutate(Status=tbVsControlExprMat$Status)

ggplot(tbVsControlUmapDF, aes(x=X1, y=X2, colour=Status)) +
  geom_point(size=3) +
  theme_bw() +
  labs(title="UMAP transformed mtcars",
       x="UMAP_1",
       y="UMAP_2")


```


***

# Unsupervised clustering

Unsupervised clustering tries to find natural groupings or clusters in your data. "Unsupervised" means that
you are not supplying meaningful labels with your data, clustering is done purely on your data values.


## K-means

K-means clustering breaks your data into 'k' distinct clusters (you choose 'k'). It works
by initially randomly placing 'k' cluster centers in your data. Whichever cluster center is closest to a data point is used to label that datapoint. Then it re-calculates a center point ('centroid') for each cluster, based on the cluster assignments, and re-assigns your data to the cluster with the closest centroid. This process is carried out repeatedly until the cluster assignments stabilize.

Since you have to specify 'k' up front, it's a good idea to try a few different values for 'k' and test which gives you the best clustering.

Here, we know there are two 'groups' (TB / Control) so we will use that. There are many approaches out there to determine a 'good' number of clusters for your data, but we won't discuss them here.


```{r}


kmeansClustering <- kmeans(select(tbVsControlPCAdf, -SampleID, -Status), centers=2, nstart=10, iter.max=100)
kmeansClusterDF <- data.frame(k=factor(kmeansClustering$cluster)) %>%
  mutate(SampleID=tbVsControlExprMat$SampleID)
#Now, lets visualize the clusters on the PCA

tbVsControlClustPCADF <- tbVsControlPCAdf %>%
  left_join(kmeansClusterDF, by="SampleID")

tbVsControlClustCenters <- data.frame(kmeansClustering$centers) %>%
  mutate(k=1:2)

table(tbVsControlClustPCADF$Status, tbVsControlClustPCADF$k)


```

We can visualize the cluster assignments and centroids directly, and compare them to the known TB status of the subjects.
We see that all of one cluster is TB samples, while the other cluster is a mix of TB and controls.


```{r}
ggplot(tbVsControlClustPCADF, aes(x=PC1, y=PC2)) +
  geom_point(aes(colour=k, shape=Status), size=2) +
  geom_point(data=tbVsControlClustCenters, size=5, shape=1, aes(colour=factor(k))) + 
  theme_bw() +
  scale_shape_manual(values=c(20, 3)) +
  labs(title="K-means clustering",
       x=sprintf("PC1 (%.2f%%)", tbVsControlVarianceExplained["PC1"]*100),
       y=sprintf("PC2 (%.2f%%)", tbVsControlVarianceExplained["PC2"]*100),
       caption="Cluster centers shown as empty circles")

```

## Heatmaps and hierarchical clustering

Unlike k-means, hierarchical clustering does not split your data into a fixed number of discrete clusters. Instead, it builds a tree,
i.e. a hierarchical structure. Similar data points are linked together iteratively until all points are gathered in a hierarchy that
can be visualized as a dendrogram.

The `pheatmap` package is a simple way to create hierarchically clustered heatmaps. And we can annotate it with our
kmeans clustering. Here we center and scale the column values, so we can visualize the heatmap columns effectively side-by-sidfe, despite the fact that they represent genes with very different mean expression. 

`pheatmap` uses the R `hclust` function to create hierarchical clustering of columns and rows. 

The default hierarchical clustering approach, 'complete linkage' applies a similarity measurement (default: euclidian distance) between each row/column and links the most similar row/column. This is repeated until all rows/columns are linked. When calculating distances between merged clusters, the maximum distance between any two elements of the merged clusters is used.

Hierarchical clusterings can be visualized using a dendrogram, where the height of the branches corresponds to the 'distance' between clusters.

```{r fig.width=12}
tbVsControlExprHmap <- tbVsControlExprMat %>%
  select(-Status) %>%
  column_to_rownames("SampleID")

tbVsControlHmapAnnot <- select(tbVsControlExprMat, Status, SampleID) %>%
  left_join(kmeansClusterDF) %>%
  column_to_rownames("SampleID")

pheatmap(tbVsControlExprHmap, 
         annotation_row = tbVsControlHmapAnnot,
         clustering_method="complete",
         scale="column",
         main='Hierarchical clustering using complete linkage',
         border_color = NA,
         fontsize_col = 6, fontsize_row=6) 

```

However, complete linkage is not the only approach: for example Ward's method, instead of linking by maximum distance, it joins clusters based on the minimum between cluster distance. 

```{r fig.width=12}

pheatmap(tbVsControlExprHmap, 
         annotation_row = tbVsControlHmapAnnot,
         clustering_method="ward.D2",
         scale="column",
         main='Hierarchical clustering using ward linkage',
         border_color = NA,
         fontsize_col = 6, fontsize_row=6) 

```

Or we can cluster on the average between-cluster distance

```{r fig.width=12}
pheatmap(tbVsControlExprHmap, 
         annotation_row = tbVsControlHmapAnnot,
         clustering_method="average",
         scale="column",
         main='Hierarchical clustering using average linkage',
         border_color = NA,
         fontsize_col = 6, fontsize_row=6) 

```

We can also manually calculate the hierarchical clustering using the `dist` and `hclust` functions,
and plot the clustering as a dendrogram. `dist` calculates a distance matrix (default: euclidean distance) between
rows of the expression matrix, and `hclust` determines the hierarchical clustering.

```{r fig.width=12}
rowClustering <- hclust(dist(tbVsControlExprHmap), method="ward.D2")

pheatmap(tbVsControlExprHmap, 
         annotation_row = tbVsControlHmapAnnot,
         clustering_method="average",
         cluster_rows = rowClustering,
         scale="column",
         main='Hierarchical clustering using manual row clustering',
         border_color = NA,
         fontsize_col = 6, fontsize_row=6) 

plot(rowClustering, labels=tbVsControlHmapAnnot$Status)

```

Using all these approaches, we see a similar result: a group of PTB cases that cluster together due to high expression pro-inflamatory and interferon inducible genes, while a mix of controls and other PTB cases have lower expression of these genes. This is quite similar to what the original authors report in figure 1 of their paper.

